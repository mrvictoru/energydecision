{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file and print out the data\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (270_304, 54)\n",
      "┌──────────┬───────────┬──────────┬──────────────────────┬───┬───────┬───────┬───────┬─────────────┐\n",
      "│ Customer ┆ Generator ┆ Postcode ┆ Consumption Category ┆ … ┆ 23:00 ┆ 23:30 ┆ 0:00  ┆ Row Quality │\n",
      "│ ---      ┆ Capacity  ┆ ---      ┆ ---                  ┆   ┆ ---   ┆ ---   ┆ ---   ┆ ---         │\n",
      "│ i64      ┆ ---       ┆ i64      ┆ str                  ┆   ┆ f64   ┆ f64   ┆ f64   ┆ str         │\n",
      "│          ┆ f64       ┆          ┆                      ┆   ┆       ┆       ┆       ┆             │\n",
      "╞══════════╪═══════════╪══════════╪══════════════════════╪═══╪═══════╪═══════╪═══════╪═════════════╡\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ CL                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 1.063 ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ GC                   ┆ … ┆ 0.118 ┆ 0.219 ┆ 0.162 ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ GG                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 0.0   ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ CL                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 1.075 ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ GC                   ┆ … ┆ 0.224 ┆ 0.088 ┆ 0.084 ┆ null        │\n",
      "│ …        ┆ …         ┆ …        ┆ …                    ┆ … ┆ …     ┆ …     ┆ …     ┆ …           │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GC                   ┆ … ┆ 0.517 ┆ 0.393 ┆ 0.226 ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GG                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 0.0   ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ CL                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 2.621 ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GC                   ┆ … ┆ 0.131 ┆ 0.358 ┆ 0.235 ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GG                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 0.0   ┆ null        │\n",
      "└──────────┴───────────┴──────────┴──────────────────────┴───┴───────┴───────┴───────┴─────────────┘\n",
      "['Customer', 'Generator Capacity', 'Postcode', 'Consumption Category', 'date', '0:30', '1:00', '1:30', '2:00', '2:30', '3:00', '3:30', '4:00', '4:30', '5:00', '5:30', '6:00', '6:30', '7:00', '7:30', '8:00', '8:30', '9:00', '9:30', '10:00', '10:30', '11:00', '11:30', '12:00', '12:30', '13:00', '13:30', '14:00', '14:30', '15:00', '15:30', '16:00', '16:30', '17:00', '17:30', '18:00', '18:30', '19:00', '19:30', '20:00', '20:30', '21:00', '21:30', '22:00', '22:30', '23:00', '23:30', '0:00', 'Row Quality']\n"
     ]
    }
   ],
   "source": [
    "datapath = '../data/2011-2012 Solar home electricity data v2.csv'\n",
    "# skip the first line in csv and read the next line as column\n",
    "# then read the rest of the file and store as dataframe\n",
    "df = pl.read_csv(datapath, skip_rows=1)\n",
    "print(df)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show me different values in 'Generator Capacity'\n",
    "GenCap = df['Generator Capacity'].unique()\n",
    "\n",
    "# plot histogram of 'Generator Capacity'\n",
    "plt.hist(GenCap, bins=20)\n",
    "plt.xlabel('Generator Capacity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generator Capacity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import transform_polars_df\n",
    "\n",
    "# get all the unique customers as their own dataframes\n",
    "customers = df['Customer'].unique()\n",
    "# pick 80% of the random customers as training data\n",
    "training_customers = np.random.choice(customers, int(0.8*len(customers)), replace=False)\n",
    "# the rest of the customers are testing data\n",
    "testing_customers = np.setdiff1d(customers, training_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the customers number to a csv file\n",
    "np.savetxt('../data/training_customers.csv', training_customers, fmt='%s')\n",
    "np.savetxt('../data/testing_customers.csv', testing_customers, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each customer and use transform_polars_df to get the dataframe and store it in a list call dataset\n",
    "training_dataset = []\n",
    "for customer in training_customers:\n",
    "    customer_df = df.filter(pl.col('Customer') == customer)\n",
    "    try:\n",
    "        newcustomerdf = transform_polars_df(customer_df, import_energy_price=0.15, export_energy_price=0.1, price_periods=\"7am – 10am | 4pm – 9pm\", default_import_energy_price=0.1, default_export_energy_price=0.08)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with customer as training dataset: {customer}\")\n",
    "        print(e)\n",
    "        break\n",
    "    training_dataset.append(newcustomerdf)\n",
    "\n",
    "testing_dataset = []\n",
    "for customer in testing_customers:\n",
    "    customer_df = df.filter(pl.col('Customer') == customer)\n",
    "    try:\n",
    "        newcustomerdf = transform_polars_df(customer_df, import_energy_price=0.15, export_energy_price=0.1, price_periods=\"7am – 10am | 4pm – 9pm\", default_import_energy_price=0.1, default_export_energy_price=0.08)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with customer as testing dataset: {customer}\")\n",
    "        print(e)\n",
    "        break\n",
    "    testing_dataset.append(newcustomerdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps possible in training dataset: 4216080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from EnergySimEnv import SolarBatteryEnv\n",
    "\n",
    "\n",
    "# Helper: create an environment instance from a dataset.\n",
    "def make_env(dataset):\n",
    "    def _init():\n",
    "        env = SolarBatteryEnv(dataset, max_step=len(dataset)-1)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Create a list of environment creation functions to build a vectorized environment.\n",
    "env_fns = [make_env(ds) for ds in training_dataset]\n",
    "vec_env = DummyVecEnv(env_fns)\n",
    "\n",
    "num_total_steps = len(training_dataset[0])*len(training_dataset)\n",
    "print(f\"Total number of steps possible in training dataset: {num_total_steps}\")\n",
    "\n",
    "testing_env_fns = [make_env(ds) for ds in testing_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def optimize_ppo(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
    "    clip_range = trial.suggest_uniform(\"clip_range\", 0.1, 0.3)\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
    "    \n",
    "    # Choose network architecture preset\n",
    "    net_arch_choice = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"])\n",
    "    if net_arch_choice == \"small\":\n",
    "        net_arch = [64, 64]\n",
    "    elif net_arch_choice == \"medium\":\n",
    "        net_arch = [256, 256]\n",
    "    else:\n",
    "        net_arch = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=net_arch)\n",
    "    \n",
    "    # Instantiate PPO with the trial hyperparameters\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        clip_range=clip_range,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        policy_kwargs=policy_kwargs\n",
    "    )\n",
    "    \n",
    "    # Train for a small number of timesteps for tuning\n",
    "    model.learn(total_timesteps=40000)\n",
    "    \n",
    "    # Evaluate on the testing environment (Monitor-wrapped)\n",
    "    mean_reward, _ = evaluate_policy(model, Monitor(testing_env_fns[0]()), n_eval_episodes=3, deterministic=False)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimize_ppo, n_trials=20)\n",
    "print(\"Best PPO trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 16402  |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 29     |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4846          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 983040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5401504e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.16e+28      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -1.5e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.58e+30      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3910         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 377          |\n",
      "|    total_timesteps      | 1474560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.455918e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.11e+28     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -2.51e-08    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.12e+31     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3559          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 552           |\n",
      "|    total_timesteps      | 1966080       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8050196e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.15e+31      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -1.18e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.27e+30      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3378          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 727           |\n",
      "|    total_timesteps      | 2457600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4817474e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.05e+30      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 1.01e-08      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.04e+31      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3273          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 900           |\n",
      "|    total_timesteps      | 2949120       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4737436e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.25e+31      |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -7.51e-09     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.7e+30       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3202         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 1074         |\n",
      "|    total_timesteps      | 3440640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.558307e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.44e+31     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -4.56e-09    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.14e+31     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3151          |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 1247          |\n",
      "|    total_timesteps      | 3932160       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0003214e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.54e+28      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -2.03e-07     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.08e+31      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3114          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 1420          |\n",
      "|    total_timesteps      | 4423680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4791203e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.65e+30      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -3.68e-08     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.03e+30      |\n",
      "-------------------------------------------\n",
      "Using cpu device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17528    |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 4.59e+04 |\n",
      "|    std                | 0.956    |\n",
      "|    value_loss         | 2.26e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17642    |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 4.02e+04 |\n",
      "|    std                | 0.94     |\n",
      "|    value_loss         | 2.08e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17684    |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 4.17e+04 |\n",
      "|    std                | 0.939    |\n",
      "|    value_loss         | 2.15e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17715    |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 4.09e+04 |\n",
      "|    std                | 0.935    |\n",
      "|    value_loss         | 2.15e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17740    |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 4.03e+04 |\n",
      "|    std                | 0.934    |\n",
      "|    value_loss         | 2.15e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17788    |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 3.72e+04 |\n",
      "|    std                | 0.919    |\n",
      "|    value_loss         | 2.02e+09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17797     |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 840000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.33     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 2.19e+07  |\n",
      "|    std                | 0.917     |\n",
      "|    value_loss         | 2.27e+16  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17785    |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 960000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.06e+06 |\n",
      "|    std                | 0.897    |\n",
      "|    value_loss         | 3.67e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17795    |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 3.72e+04 |\n",
      "|    std                | 0.887    |\n",
      "|    value_loss         | 2.07e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17763    |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 2.4e+06  |\n",
      "|    std                | 0.881    |\n",
      "|    value_loss         | 9.06e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17767    |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 3.76e+04 |\n",
      "|    std                | 0.884    |\n",
      "|    value_loss         | 2e+09    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17777    |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 4.18e+04 |\n",
      "|    std                | 0.884    |\n",
      "|    value_loss         | 2.32e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17787    |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 1560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 3.59e+04 |\n",
      "|    std                | 0.884    |\n",
      "|    value_loss         | 2.05e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17799    |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 3.99e+04 |\n",
      "|    std                | 0.887    |\n",
      "|    value_loss         | 2.16e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17807    |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 101      |\n",
      "|    total_timesteps    | 1800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 3.75e+04 |\n",
      "|    std                | 0.885    |\n",
      "|    value_loss         | 2.03e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17814    |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 4.19e+04 |\n",
      "|    std                | 0.884    |\n",
      "|    value_loss         | 2.33e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17828    |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 2040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 3.8e+04  |\n",
      "|    std                | 0.889    |\n",
      "|    value_loss         | 2.04e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17838    |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 2160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 4.1e+04  |\n",
      "|    std                | 0.89     |\n",
      "|    value_loss         | 2.27e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17844    |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 2280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 3.98e+04 |\n",
      "|    std                | 0.896    |\n",
      "|    value_loss         | 2.05e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17854    |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 2400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 4.32e+04 |\n",
      "|    std                | 0.878    |\n",
      "|    value_loss         | 2.31e+09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17860     |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 2520000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.29     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 3.44e+04  |\n",
      "|    std                | 0.877     |\n",
      "|    value_loss         | 1.89e+09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17862    |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 147      |\n",
      "|    total_timesteps    | 2640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 4.4e+04  |\n",
      "|    std                | 0.879    |\n",
      "|    value_loss         | 2.34e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17863    |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 154      |\n",
      "|    total_timesteps    | 2760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 3.5e+04  |\n",
      "|    std                | 0.88     |\n",
      "|    value_loss         | 1.86e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17868    |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 161      |\n",
      "|    total_timesteps    | 2880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 4.35e+04 |\n",
      "|    std                | 0.866    |\n",
      "|    value_loss         | 2.38e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17873    |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 3000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 3.52e+04 |\n",
      "|    std                | 0.872    |\n",
      "|    value_loss         | 2.01e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17880    |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 174      |\n",
      "|    total_timesteps    | 3120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 4.06e+04 |\n",
      "|    std                | 0.868    |\n",
      "|    value_loss         | 2.26e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17885    |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 3240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 3.41e+04 |\n",
      "|    std                | 0.87     |\n",
      "|    value_loss         | 2.05e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17892    |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 187      |\n",
      "|    total_timesteps    | 3360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 3.93e+04 |\n",
      "|    std                | 0.859    |\n",
      "|    value_loss         | 2.25e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17896    |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 194      |\n",
      "|    total_timesteps    | 3480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 3.41e+04 |\n",
      "|    std                | 0.866    |\n",
      "|    value_loss         | 1.94e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17901    |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 201      |\n",
      "|    total_timesteps    | 3600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 4.24e+04 |\n",
      "|    std                | 0.868    |\n",
      "|    value_loss         | 2.38e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17905    |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 207      |\n",
      "|    total_timesteps    | 3720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 3.53e+04 |\n",
      "|    std                | 0.864    |\n",
      "|    value_loss         | 1.92e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17910    |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 214      |\n",
      "|    total_timesteps    | 3840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 4.34e+04 |\n",
      "|    std                | 0.865    |\n",
      "|    value_loss         | 2.37e+09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17908     |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 221       |\n",
      "|    total_timesteps    | 3960000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 3.73e+04  |\n",
      "|    std                | 0.859     |\n",
      "|    value_loss         | 2.08e+09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17906    |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 227      |\n",
      "|    total_timesteps    | 4080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 4.04e+04 |\n",
      "|    std                | 0.853    |\n",
      "|    value_loss         | 2.27e+09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17905    |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 234      |\n",
      "|    total_timesteps    | 4200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 3.79e+04 |\n",
      "|    std                | 0.862    |\n",
      "|    value_loss         | 2.13e+09 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best parameters from the Optuna study\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Determine network architecture based on the best net_arch choice\n",
    "if best_params[\"net_arch\"] == \"small\":\n",
    "    net_arch = [64, 64]\n",
    "elif best_params[\"net_arch\"] == \"medium\":\n",
    "    net_arch = [256, 256]\n",
    "else:\n",
    "    net_arch = [400, 300]\n",
    "\n",
    "policy_kwargs = dict(net_arch=net_arch)\n",
    "\n",
    "# Instantiate PPO model with best parameters\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    clip_range=best_params[\"clip_range\"],\n",
    "    ent_coef=best_params[\"ent_coef\"],\n",
    "    vf_coef=best_params[\"vf_coef\"],\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "# Optionally, evaluate before training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    ppo_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False\n",
    ")\n",
    "evaluation_results['PPO_pre_training'] = {\n",
    "    'mean_reward': mean_reward,\n",
    "    'std_reward': std_reward\n",
    "}\n",
    "\n",
    "# Train the model using the tuned hyperparameters\n",
    "ppo_model.learn(total_timesteps=num_total_steps)\n",
    "\n",
    "# Evaluate after training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    ppo_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False\n",
    ")\n",
    "evaluation_results['PPO_post_training'] = {\n",
    "    'mean_reward': mean_reward,\n",
    "    'std_reward': std_reward\n",
    "}\n",
    "\n",
    "# Save the trained model\n",
    "ppo_model.save(\"ppo_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def optimize_a2c(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
    "    \n",
    "    net_arch_choice = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"])\n",
    "    if net_arch_choice == \"small\":\n",
    "        net_arch = [64, 64]\n",
    "    elif net_arch_choice == \"medium\":\n",
    "        net_arch = [256, 256]\n",
    "    else:\n",
    "        net_arch = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=net_arch)\n",
    "    \n",
    "    model = A2C(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        policy_kwargs=policy_kwargs\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=40000)\n",
    "    \n",
    "    mean_reward, _ = evaluate_policy(model, Monitor(testing_env_fns[0]()), n_eval_episodes=3, deterministic=False)\n",
    "    return mean_reward\n",
    "\n",
    "study_a2c = optuna.create_study(direction=\"maximize\")\n",
    "study_a2c.optimize(optimize_a2c, n_trials=20)\n",
    "print(\"Best A2C trial:\", study_a2c.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve best parameters from the A2C Optuna study (assumed to be stored in study_a2c)\n",
    "best_params_a2c = study_a2c.best_trial.params\n",
    "\n",
    "# Determine network architecture based on the best net_arch choice\n",
    "if best_params_a2c[\"net_arch\"] == \"small\":\n",
    "    net_arch = [64, 64]\n",
    "elif best_params_a2c[\"net_arch\"] == \"medium\":\n",
    "    net_arch = [256, 256]\n",
    "else:\n",
    "    net_arch = [400, 300]\n",
    "\n",
    "policy_kwargs = dict(net_arch=net_arch)\n",
    "\n",
    "# Instantiate A2C model with best parameters\n",
    "a2c_model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=best_params_a2c[\"learning_rate\"],\n",
    "    gamma=best_params_a2c[\"gamma\"],\n",
    "    ent_coef=best_params_a2c[\"ent_coef\"],\n",
    "    vf_coef=best_params_a2c[\"vf_coef\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Evaluate before training\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['A2C_pre_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "\n",
    "# Train the A2C model\n",
    "a2c_model.learn(total_timesteps=num_total_steps)\n",
    "\n",
    "# Evaluate after training\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['A2C_post_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "\n",
    "# Save the trained model\n",
    "a2c_model.save(\"a2c_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 14:36:07,507] A new study created in memory with name: no-name-53ddfe20-5ee5-4597-a9c7-9e026d17f025\n",
      "/tmp/ipykernel_180/1007132315.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_180/1007132315.py:17: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  tau = trial.suggest_uniform(\"tau\", 0.001, 0.02)\n",
      "/tmp/ipykernel_180/1007132315.py:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "[I 2025-03-26 14:36:12,168] Trial 0 finished with value: -330086.67074366665 and parameters: {'learning_rate': 3.0420771142850994e-05, 'tau': 0.0048178460406587496, 'gamma': 0.9524958380749334, 'batch_size': 128, 'net_arch': 'large'}. Best is trial 0 with value: -330086.67074366665.\n",
      "[I 2025-03-26 14:36:16,328] Trial 1 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0008419858411981373, 'tau': 0.00787223268857461, 'gamma': 0.9885992818729119, 'batch_size': 64, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:20,400] Trial 2 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0014185949727188799, 'tau': 0.009700274889341859, 'gamma': 0.9416777787446022, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:24,442] Trial 3 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0019217210085488857, 'tau': 0.004653921083673531, 'gamma': 0.9428553129836782, 'batch_size': 256, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:28,506] Trial 4 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.001168967317132838, 'tau': 0.014306787087924159, 'gamma': 0.959789898927376, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:32,750] Trial 5 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.00011506978711268606, 'tau': 0.008832114975259587, 'gamma': 0.9092682264207848, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:36,882] Trial 6 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0007556624919455673, 'tau': 0.011506138865478945, 'gamma': 0.9218938132691364, 'batch_size': 64, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:41,078] Trial 7 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.000836665728712073, 'tau': 0.0016645442512468705, 'gamma': 0.9035299058506988, 'batch_size': 128, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:45,160] Trial 8 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0010899481552003312, 'tau': 0.009817469819796969, 'gamma': 0.9586991883369996, 'batch_size': 128, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:49,330] Trial 9 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.006491042477128918, 'tau': 0.001110384502369038, 'gamma': 0.9504019189158186, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:53,440] Trial 10 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.00013932015802953302, 'tau': 0.019637332629143862, 'gamma': 0.9960128543928889, 'batch_size': 64, 'net_arch': 'medium'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:36:57,481] Trial 11 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0069612333887681226, 'tau': 0.013748228963085318, 'gamma': 0.9910092652761865, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:01,509] Trial 12 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.00035614475859676464, 'tau': 0.006594761198970978, 'gamma': 0.9766572676368611, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:05,648] Trial 13 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.0028100698630180152, 'tau': 0.007231761016254995, 'gamma': 0.9313925733955186, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:09,817] Trial 14 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.00025761803576085726, 'tau': 0.012499239446872385, 'gamma': 0.976336940596253, 'batch_size': 64, 'net_arch': 'medium'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:13,921] Trial 15 finished with value: 9864251.889012666 and parameters: {'learning_rate': 2.014461028435631e-05, 'tau': 0.01659432740208443, 'gamma': 0.9715727589812547, 'batch_size': 64, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:18,073] Trial 16 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.0025067426982367936, 'tau': 0.008374929573498138, 'gamma': 0.935764596450336, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:22,255] Trial 17 finished with value: -330086.67074366665 and parameters: {'learning_rate': 0.0003714721312245855, 'tau': 0.004694879945020401, 'gamma': 0.9209271264687685, 'batch_size': 64, 'net_arch': 'large'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:26,324] Trial 18 finished with value: 9864251.889012666 and parameters: {'learning_rate': 0.0037559052642454375, 'tau': 0.010177331915621916, 'gamma': 0.965115767099184, 'batch_size': 128, 'net_arch': 'small'}. Best is trial 1 with value: 9864251.889012666.\n",
      "[I 2025-03-26 14:37:30,328] Trial 19 finished with value: 9864251.889012666 and parameters: {'learning_rate': 5.039884548893261e-05, 'tau': 0.015476455166602916, 'gamma': 0.9406987174800573, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 1 with value: 9864251.889012666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value:  9864251.889012666\n",
      "Params: \n",
      "  learning_rate: 0.0008419858411981373\n",
      "  tau: 0.00787223268857461\n",
      "  gamma: 0.9885992818729119\n",
      "  batch_size: 64\n",
      "  net_arch: large\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "n_actions = vec_env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(\n",
    "    mean=np.zeros(n_actions), \n",
    "    sigma=0.2 * np.ones(n_actions)\n",
    ")\n",
    "\n",
    "def optimize_ddpg(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    tau = trial.suggest_uniform(\"tau\", 0.001, 0.02)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    # Choose network architecture preset\n",
    "    net_arch_choice = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"])\n",
    "    if net_arch_choice == \"small\":\n",
    "        net_arch = [64, 64]\n",
    "    elif net_arch_choice == \"medium\":\n",
    "        net_arch = [256, 256]\n",
    "    else:\n",
    "        net_arch = [400, 300]\n",
    "        \n",
    "    policy_kwargs = dict(net_arch=net_arch)\n",
    "    \n",
    "    # Create DDPG model with sampled hyperparameters.\n",
    "    model = DDPG(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        action_noise=action_noise\n",
    "    )\n",
    "    \n",
    "    # Use fewer timesteps for speed; adjust as needed.\n",
    "    model.learn(total_timesteps=40000)\n",
    "    \n",
    "    # Evaluate the policy on a Monitor-wrapped environment.\n",
    "    mean_reward, _ = evaluate_policy(model, Monitor(testing_env_fns[0]()), n_eval_episodes=3, deterministic=True)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimize_ddpg, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best parameters from the study\n",
    "best_trial = study.best_trial\n",
    "\n",
    "learning_rate = best_trial.params[\"learning_rate\"]\n",
    "tau = best_trial.params[\"tau\"]\n",
    "gamma = best_trial.params[\"gamma\"]\n",
    "batch_size = best_trial.params[\"batch_size\"]\n",
    "net_arch_choice = best_trial.params[\"net_arch\"]\n",
    "\n",
    "if net_arch_choice == \"small\":\n",
    "    net_arch = [64, 64]\n",
    "elif net_arch_choice == \"medium\":\n",
    "    net_arch = [256, 256]\n",
    "else:\n",
    "    net_arch = [400, 300]\n",
    "\n",
    "policy_kwargs = dict(net_arch=net_arch)\n",
    "\n",
    "# Initialize action noise\n",
    "n_actions = vec_env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(\n",
    "    mean=np.zeros(n_actions), \n",
    "    sigma=0.2 * np.ones(n_actions)  # Adjust sigma to tune exploration\n",
    ")\n",
    "\n",
    "# Instantiate DDPG model with the best hyperparameters\n",
    "ddpg_model = DDPG(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=learning_rate,\n",
    "    tau=tau,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise\n",
    ")\n",
    "# Train third agent with DDPG\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(ddpg_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['DDPG_pre_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "ddpg_model.learn(total_timesteps=num_total_steps)\n",
    "mean_reward, std_reward = evaluate_policy(ddpg_model, Monitor(env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['DDPG_post_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "ddpg_model.save(\"ddpg_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PPO_pre_training': {'mean_reward': np.float64(67276190688174.2), 'std_reward': np.float64(134552381244810.45)}, 'PPO_post_training': {'mean_reward': np.float64(-1.0809832116947158e+16), 'std_reward': np.float64(2.1619664233248996e+16)}, 'A2C_pre_training': {'mean_reward': np.float64(648765.6539544), 'std_reward': np.float64(2020622.6794400855)}, 'A2C_post_training': {'mean_reward': np.float64(9874790.2043012), 'std_reward': np.float64(31614.94586559981)}, 'DDPG_pre_training': {'mean_reward': np.float64(9874790.2043012), 'std_reward': np.float64(31614.94586559981)}, 'DDPG_post_training': {'mean_reward': np.float64(67131379.4049546), 'std_reward': np.float64(31615.064492797854)}}\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, algorithm='rule', model=None):\n",
    "        \"\"\"\n",
    "        env: an instance of SolarBatteryEnv.\n",
    "        algorithm: choose between 'rule' for a rule‑based agent or 'rl' for reinforcement learning.\n",
    "        model: For RL algorithm, a trained model with a predict method (e.g., from stable_baselines3).\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.algorithm = algorithm.lower()\n",
    "        self.model = model\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        if self.algorithm == 'rule':\n",
    "            return self.rule_based_action(obs)\n",
    "        elif self.algorithm == 'rl':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"RL algorithm selected but no model provided.\")\n",
    "            # Ensure observation has the right shape for the model (e.g., batch dimension)\n",
    "            obs_batch = obs[None, ...] if isinstance(obs, np.ndarray) else obs\n",
    "            action, _ = self.model.predict(obs_batch, deterministic=True)\n",
    "            # Remove batch dimension if applicable.\n",
    "            return action[0] if isinstance(action, np.ndarray) and action.ndim > 1 else action\n",
    "        elif self.algorithm == 'dt':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"Decision Transformer selected but no model provided.\")\n",
    "            # Prepare inputs for the Decision Transformer.\n",
    "            # Assumes the current observation corresponds to a single timestep.\n",
    "            device = next(self.model.parameters()).device\n",
    "            state = torch.tensor(obs, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
    "            # Dummy return-to-go (rtg), here as 0; this should be set appropriately.\n",
    "            rtg = torch.tensor([[0.0]], dtype=torch.float32, device=device).reshape(1, 1, 1)\n",
    "            # Dummy timestep (set to 0 for the first step, update as needed).\n",
    "            timestep = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "            # Dummy previous action: zeros with the correct dimension. It is assumed that your model has an attribute act_dim.\n",
    "            actions = torch.zeros((1, 1, self.model.act_dim), dtype=torch.float32, device=device)\n",
    "            # Forward pass through the Decision Transformer.\n",
    "            _, _, act_preds = self.model(state, rtg, timestep, actions)\n",
    "            # Extract action from the predicted actions.\n",
    "            action = act_preds[0, 0].detach().cpu().numpy().tolist()\n",
    "            return action\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Algorithm '{self.algorithm}' is not supported.\")\n",
    "\n",
    "    def rule_based_action(self, obs):\n",
    "        \"\"\"\n",
    "        Simple heuristic based on the battery level. It assumes that the observation's\n",
    "        second-to-last element corresponds to the battery level.\n",
    "        \"\"\"\n",
    "        battery_level = obs[-2]\n",
    "        capacity = self.env.battery_capacity\n",
    "        # Rule: If battery level is below 20% of capacity, charge;\n",
    "        #       if above 80%, discharge; otherwise, hold.\n",
    "        if battery_level < capacity * 0.2:\n",
    "            return [1.0]  # Full charge action.\n",
    "        elif battery_level > capacity * 0.8:\n",
    "            return [-1.0]  # Full discharge action.\n",
    "        else:\n",
    "            return [0.0]  # Hold / no operation.\n",
    "\n",
    "    def run_episode(self, render=False):\n",
    "        \"\"\"\n",
    "        Runs one episode on the environment.\n",
    "        render: if True, call env.render() after every step.\n",
    "        Returns a Polars DataFrame with the observations, actions, and rewards.\n",
    "        \"\"\"\n",
    "        obs, _ = self.env.reset()\n",
    "        logs = []\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            action = self.choose_action(obs)\n",
    "            next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            logs.append({\n",
    "                'observation': obs.tolist() if isinstance(obs, np.ndarray) else obs,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'info': info\n",
    "            })\n",
    "            obs = next_obs\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "        return pl.DataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = testing_env_fns[0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ddpg model and create an agent\n",
    "#ddpg_model = DDPG.load(\"ddpg_agent\")\n",
    "agent = Agent(env, algorithm='rl', model=ppo_model)\n",
    "\n",
    "# run an episode with the agent\n",
    "episode_logs = agent.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Timestamp', 'SolarGen', 'HouseLoad', 'FutureSolar', 'FutureLoad', 'ImportEnergyPrice', 'ExportEnergyPrice', 'Time', 'BatteryLevel', 'GridFlow']\n",
      "shape: (9,)\n",
      "Series: '' [f64]\n",
      "[\n",
      "\t1.3095e15\n",
      "\t0.0\n",
      "\t0.623\n",
      "\t0.0\n",
      "\t0.034\n",
      "\t0.1\n",
      "\t0.08\n",
      "\t5.0\n",
      "\t0.0\n",
      "]\n",
      "-0.003123288741335273\n",
      "{'reward_info': {'battery_charge': 0, 'battery_discharge': 0.007808221969753504, 'demand': 0.623, 'supply': 0.007808221969753504, 'grid_energy': 0.6151918172836304, 'energy_price': 0.08, 'grid_violation_penalty': 0, 'grid_reward': -0.04921534284949303, 'battery_deg_penalty': 0.009673118591308594, 'battery_life_cost': 15300, 'final_reward': -148.0479278564453}}\n"
     ]
    }
   ],
   "source": [
    "print(env.get_observation_header())\n",
    "step = 0\n",
    "print(episode_logs[step][\"observation\"][0])\n",
    "print(episode_logs[step][\"action\"][0])\n",
    "print(episode_logs[step][\"info\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
