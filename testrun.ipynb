{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file and print out the data\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from helper import transform_polars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (270_304, 54)\n",
      "┌──────────┬───────────┬──────────┬──────────────────────┬───┬───────┬───────┬───────┬─────────────┐\n",
      "│ Customer ┆ Generator ┆ Postcode ┆ Consumption Category ┆ … ┆ 23:00 ┆ 23:30 ┆ 0:00  ┆ Row Quality │\n",
      "│ ---      ┆ Capacity  ┆ ---      ┆ ---                  ┆   ┆ ---   ┆ ---   ┆ ---   ┆ ---         │\n",
      "│ i64      ┆ ---       ┆ i64      ┆ str                  ┆   ┆ f64   ┆ f64   ┆ f64   ┆ str         │\n",
      "│          ┆ f64       ┆          ┆                      ┆   ┆       ┆       ┆       ┆             │\n",
      "╞══════════╪═══════════╪══════════╪══════════════════════╪═══╪═══════╪═══════╪═══════╪═════════════╡\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ CL                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 1.063 ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ GC                   ┆ … ┆ 0.118 ┆ 0.219 ┆ 0.162 ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ GG                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 0.0   ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ CL                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 1.075 ┆ null        │\n",
      "│ 1        ┆ 3.78      ┆ 2076     ┆ GC                   ┆ … ┆ 0.224 ┆ 0.088 ┆ 0.084 ┆ null        │\n",
      "│ …        ┆ …         ┆ …        ┆ …                    ┆ … ┆ …     ┆ …     ┆ …     ┆ …           │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GC                   ┆ … ┆ 0.517 ┆ 0.393 ┆ 0.226 ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GG                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 0.0   ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ CL                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 2.621 ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GC                   ┆ … ┆ 0.131 ┆ 0.358 ┆ 0.235 ┆ null        │\n",
      "│ 300      ┆ 3.36      ┆ 2086     ┆ GG                   ┆ … ┆ 0.0   ┆ 0.0   ┆ 0.0   ┆ null        │\n",
      "└──────────┴───────────┴──────────┴──────────────────────┴───┴───────┴───────┴───────┴─────────────┘\n",
      "['Customer', 'Generator Capacity', 'Postcode', 'Consumption Category', 'date', '0:30', '1:00', '1:30', '2:00', '2:30', '3:00', '3:30', '4:00', '4:30', '5:00', '5:30', '6:00', '6:30', '7:00', '7:30', '8:00', '8:30', '9:00', '9:30', '10:00', '10:30', '11:00', '11:30', '12:00', '12:30', '13:00', '13:30', '14:00', '14:30', '15:00', '15:30', '16:00', '16:30', '17:00', '17:30', '18:00', '18:30', '19:00', '19:30', '20:00', '20:30', '21:00', '21:30', '22:00', '22:30', '23:00', '23:30', '0:00', 'Row Quality']\n"
     ]
    }
   ],
   "source": [
    "datapath = '../data/2011-2012 Solar home electricity data v2.csv'\n",
    "# skip the first line in csv and read the next line as column\n",
    "# then read the rest of the file and store as dataframe\n",
    "df = pl.read_csv(datapath, skip_rows=1)\n",
    "print(df)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show me different values in 'Generator Capacity'\n",
    "GenCap = df['Generator Capacity'].unique()\n",
    "\n",
    "# plot histogram of 'Generator Capacity'\n",
    "plt.hist(GenCap, bins=20)\n",
    "plt.xlabel('Generator Capacity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generator Capacity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get all the unique customers as their own dataframes\n",
    "customers = df['Customer'].unique()\n",
    "# pick 80% of the random customers as training data\n",
    "training_customers = np.random.choice(customers, int(0.8*len(customers)), replace=False)\n",
    "# the rest of the customers are testing data\n",
    "testing_customers = np.setdiff1d(customers, training_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the customers number to a csv file\n",
    "np.savetxt('../data/training_customers.csv', training_customers, fmt='%s')\n",
    "np.savetxt('../data/testing_customers.csv', testing_customers, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, we can get the training and testing customers from the csv file\n",
    "training_customers = np.loadtxt('../data/training_customers.csv', dtype=int)\n",
    "testing_customers = np.loadtxt('../data/testing_customers.csv', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each customer and use transform_polars_df to get the dataframe and store it in a list call dataset\n",
    "training_dataset = []\n",
    "for customer in training_customers:\n",
    "    customer_df = df.filter(pl.col('Customer') == customer)\n",
    "    try:\n",
    "        newcustomerdf = transform_polars_df(customer_df, import_energy_price=0.15, export_energy_price=0.1, price_periods=\"7am – 10am | 4pm – 9pm\", default_import_energy_price=0.1, default_export_energy_price=0.08)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with customer as training dataset: {customer}\")\n",
    "        print(e)\n",
    "        break\n",
    "    training_dataset.append(newcustomerdf)\n",
    "\n",
    "testing_dataset = []\n",
    "for customer in testing_customers:\n",
    "    customer_df = df.filter(pl.col('Customer') == customer)\n",
    "    try:\n",
    "        newcustomerdf = transform_polars_df(customer_df, import_energy_price=0.15, export_energy_price=0.1, price_periods=\"7am – 10am | 4pm – 9pm\", default_import_energy_price=0.1, default_export_energy_price=0.08)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with customer as testing dataset: {customer}\")\n",
    "        print(e)\n",
    "        break\n",
    "    testing_dataset.append(newcustomerdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps possible in training dataset: 4216080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from EnergySimEnv import SolarBatteryEnv\n",
    "\n",
    "\n",
    "# Helper: create an environment instance from a dataset.\n",
    "def make_env(dataset):\n",
    "    def _init():\n",
    "        env = SolarBatteryEnv(dataset, max_step=len(dataset)-1)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Create a list of environment creation functions to build a vectorized environment.\n",
    "env_fns = [make_env(ds) for ds in training_dataset]\n",
    "vec_env = DummyVecEnv(env_fns)\n",
    "\n",
    "num_total_steps = len(training_dataset[0])*len(training_dataset)\n",
    "print(f\"Total number of steps possible in training dataset: {num_total_steps}\")\n",
    "\n",
    "testing_env_fns = [make_env(ds) for ds in testing_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 15:40:13,830] A new study created in memory with name: no-name-3cbc8dc5-7b3a-4166-bbb2-f9c4dbdb315c\n",
      "/tmp/ipykernel_399/1694649278.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "/tmp/ipykernel_399/1694649278.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
      "/tmp/ipykernel_399/1694649278.py:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  clip_range = trial.suggest_uniform(\"clip_range\", 0.1, 0.3)\n",
      "/tmp/ipykernel_399/1694649278.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
      "/tmp/ipykernel_399/1694649278.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "[I 2025-03-27 15:45:25,418] Trial 0 finished with value: -1689.7654173333333 and parameters: {'learning_rate': 0.0002158238297598978, 'gamma': 0.9960335507890099, 'clip_range': 0.18811425838049828, 'ent_coef': 8.958733504156693e-05, 'vf_coef': 0.9678062252421759, 'net_arch': 'large'}. Best is trial 0 with value: -1689.7654173333333.\n",
      "/tmp/ipykernel_399/1694649278.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "/tmp/ipykernel_399/1694649278.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
      "/tmp/ipykernel_399/1694649278.py:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  clip_range = trial.suggest_uniform(\"clip_range\", 0.1, 0.3)\n",
      "/tmp/ipykernel_399/1694649278.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
      "/tmp/ipykernel_399/1694649278.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
      "[I 2025-03-27 15:50:09,054] Trial 1 finished with value: -1284.7279993333334 and parameters: {'learning_rate': 2.2789073827739208e-05, 'gamma': 0.9012012028733184, 'clip_range': 0.2281389127210361, 'ent_coef': 5.767695382654587e-06, 'vf_coef': 0.6138914353013994, 'net_arch': 'medium'}. Best is trial 1 with value: -1284.7279993333334.\n",
      "[I 2025-03-27 15:54:48,880] Trial 2 finished with value: -1034.8892443333334 and parameters: {'learning_rate': 0.00025243755051833886, 'gamma': 0.9943108943433173, 'clip_range': 0.10463118010931433, 'ent_coef': 9.2741791899992e-08, 'vf_coef': 0.31580524031735013, 'net_arch': 'medium'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 15:59:23,713] Trial 3 finished with value: -1118.7516533333335 and parameters: {'learning_rate': 1.2590971092350135e-05, 'gamma': 0.9125377277097242, 'clip_range': 0.10004001868583817, 'ent_coef': 3.3154322777807473e-06, 'vf_coef': 0.9128116288036174, 'net_arch': 'small'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:04:11,911] Trial 4 finished with value: -1046.1354286666667 and parameters: {'learning_rate': 0.0003303798232683755, 'gamma': 0.9198314955233052, 'clip_range': 0.230879817019763, 'ent_coef': 1.0226600678228951e-06, 'vf_coef': 0.4273927875122563, 'net_arch': 'large'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:08:44,167] Trial 5 finished with value: -1374.4414123333333 and parameters: {'learning_rate': 2.1336115371493006e-05, 'gamma': 0.9049584351540686, 'clip_range': 0.12725222709060585, 'ent_coef': 2.0496799407874975e-08, 'vf_coef': 0.9577362001533577, 'net_arch': 'medium'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:13:33,511] Trial 6 finished with value: -1048.375778 and parameters: {'learning_rate': 5.792702324704808e-05, 'gamma': 0.9789582239841903, 'clip_range': 0.18157370528799455, 'ent_coef': 1.9425663872763224e-08, 'vf_coef': 0.1423347356423634, 'net_arch': 'small'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:17:49,358] Trial 7 finished with value: -1067.4853963333333 and parameters: {'learning_rate': 1.7289232332968185e-05, 'gamma': 0.9024740790226207, 'clip_range': 0.16581812474132615, 'ent_coef': 0.0002741082505869418, 'vf_coef': 0.166657434605892, 'net_arch': 'medium'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:22:31,188] Trial 8 finished with value: -1060.4008176666666 and parameters: {'learning_rate': 0.0001224366379367981, 'gamma': 0.9084283115251286, 'clip_range': 0.24931802162841168, 'ent_coef': 6.038427128826283e-08, 'vf_coef': 0.4629900721099627, 'net_arch': 'small'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:27:11,066] Trial 9 finished with value: -1040.8545523333332 and parameters: {'learning_rate': 2.341545677708471e-05, 'gamma': 0.975079331489992, 'clip_range': 0.18647881613464568, 'ent_coef': 6.373011457514533e-07, 'vf_coef': 0.8254629667496068, 'net_arch': 'large'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:31:16,059] Trial 10 finished with value: -2008.52361 and parameters: {'learning_rate': 0.0005878518375919947, 'gamma': 0.9415686578098679, 'clip_range': 0.2954189801231237, 'ent_coef': 0.005179377996265819, 'vf_coef': 0.25877301172777106, 'net_arch': 'medium'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:36:11,081] Trial 11 finished with value: -1064.566632 and parameters: {'learning_rate': 6.211312625934158e-05, 'gamma': 0.9726377150828004, 'clip_range': 0.15104038417003002, 'ent_coef': 2.694458496800993e-07, 'vf_coef': 0.6862086953976738, 'net_arch': 'large'}. Best is trial 2 with value: -1034.8892443333334.\n",
      "[I 2025-03-27 16:40:30,090] Trial 12 finished with value: -1027.5576466666666 and parameters: {'learning_rate': 0.0008309424785098162, 'gamma': 0.9987734869351891, 'clip_range': 0.10690975189670254, 'ent_coef': 3.09053146493795e-07, 'vf_coef': 0.7211672136584982, 'net_arch': 'large'}. Best is trial 12 with value: -1027.5576466666666.\n",
      "[I 2025-03-27 16:46:05,491] Trial 13 finished with value: -1255.0994533333333 and parameters: {'learning_rate': 0.0008614036108988721, 'gamma': 0.9988160208589895, 'clip_range': 0.11291089282943247, 'ent_coef': 1.2716080389025996e-07, 'vf_coef': 0.327425471234153, 'net_arch': 'medium'}. Best is trial 12 with value: -1027.5576466666666.\n",
      "[I 2025-03-27 16:50:33,269] Trial 14 finished with value: -2640.091515 and parameters: {'learning_rate': 0.00039268852241736905, 'gamma': 0.9568392432421373, 'clip_range': 0.13633264053415567, 'ent_coef': 3.78720103206249e-05, 'vf_coef': 0.7299913778612083, 'net_arch': 'large'}. Best is trial 12 with value: -1027.5576466666666.\n",
      "[I 2025-03-27 16:55:24,184] Trial 15 finished with value: -1319.9468603333332 and parameters: {'learning_rate': 0.00017676290385548282, 'gamma': 0.9868321132983906, 'clip_range': 0.13526182115721952, 'ent_coef': 1.6561168672594242e-06, 'vf_coef': 0.48101161748517623, 'net_arch': 'medium'}. Best is trial 12 with value: -1027.5576466666666.\n",
      "[I 2025-03-27 16:59:43,835] Trial 16 finished with value: 48892.148129 and parameters: {'learning_rate': 0.0008651400412674786, 'gamma': 0.960126711968446, 'clip_range': 0.10043810701180592, 'ent_coef': 7.78042066811534e-08, 'vf_coef': 0.5871843958999908, 'net_arch': 'large'}. Best is trial 16 with value: 48892.148129.\n",
      "[I 2025-03-27 17:04:13,483] Trial 17 finished with value: -1111.9858446666667 and parameters: {'learning_rate': 0.0008154573774534965, 'gamma': 0.9568284716121789, 'clip_range': 0.15735225271494505, 'ent_coef': 1.0229559782258385e-08, 'vf_coef': 0.5634325580517375, 'net_arch': 'large'}. Best is trial 16 with value: 48892.148129.\n",
      "[I 2025-03-27 17:08:48,353] Trial 18 finished with value: 50123.363924 and parameters: {'learning_rate': 0.0004921267105180336, 'gamma': 0.9414287353264155, 'clip_range': 0.12187518776978819, 'ent_coef': 3.1339086750943203e-07, 'vf_coef': 0.7332257033789507, 'net_arch': 'large'}. Best is trial 18 with value: 50123.363924.\n",
      "[I 2025-03-27 17:12:42,585] Trial 19 finished with value: -1626.5958953333331 and parameters: {'learning_rate': 0.0004581155361681225, 'gamma': 0.9363075656033809, 'clip_range': 0.1280494042731965, 'ent_coef': 1.6480808807949517e-05, 'vf_coef': 0.8357236852205674, 'net_arch': 'large'}. Best is trial 18 with value: 50123.363924.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PPO trial: {'learning_rate': 0.0004921267105180336, 'gamma': 0.9414287353264155, 'clip_range': 0.12187518776978819, 'ent_coef': 3.1339086750943203e-07, 'vf_coef': 0.7332257033789507, 'net_arch': 'large'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def optimize_ppo(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
    "    clip_range = trial.suggest_uniform(\"clip_range\", 0.1, 0.3)\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
    "    \n",
    "    # Choose network architecture preset\n",
    "    net_arch_choice = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"])\n",
    "    if net_arch_choice == \"small\":\n",
    "        net_arch = [64, 64]\n",
    "    elif net_arch_choice == \"medium\":\n",
    "        net_arch = [256, 256]\n",
    "    else:\n",
    "        net_arch = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=net_arch)\n",
    "    \n",
    "    # Instantiate PPO with the trial hyperparameters\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        clip_range=clip_range,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        policy_kwargs=policy_kwargs\n",
    "    )\n",
    "    \n",
    "    # Train for a small number of timesteps for tuning\n",
    "    model.learn(total_timesteps=40000)\n",
    "    \n",
    "    # Evaluate on the testing environment (Monitor-wrapped)\n",
    "    mean_reward, _ = evaluate_policy(model, Monitor(testing_env_fns[0]()), n_eval_episodes=3, deterministic=False)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimize_ppo, n_trials=20)\n",
    "print(\"Best PPO trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 18537  |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 26     |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2808          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 350           |\n",
      "|    total_timesteps      | 983040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0165119e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.122         |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.000492      |\n",
      "|    loss                 | 1.02e+28      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -3.68e-07     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.9e+30       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2305         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 639          |\n",
      "|    total_timesteps      | 1474560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.854476e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.122        |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.000492     |\n",
      "|    loss                 | 1.32e+31     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 1.96e-08     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.86e+30     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 2158           |\n",
      "|    iterations           | 4              |\n",
      "|    time_elapsed         | 911            |\n",
      "|    total_timesteps      | 1966080        |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.17284795e-08 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.122          |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.000492       |\n",
      "|    loss                 | 2.55e+29       |\n",
      "|    n_updates            | 30             |\n",
      "|    policy_gradient_loss | -2.79e-07      |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 4.46e+30       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2099          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 1170          |\n",
      "|    total_timesteps      | 2457600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7332984e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.122         |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.000492      |\n",
      "|    loss                 | 1.68e+30      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -1.54e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.01e+30      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2053          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 1435          |\n",
      "|    total_timesteps      | 2949120       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0075566e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.122         |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.000492      |\n",
      "|    loss                 | 1.48e+26      |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -1.49e-08     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.82e+30      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1994         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 1724         |\n",
      "|    total_timesteps      | 3440640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.760131e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.122        |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.000492     |\n",
      "|    loss                 | 2.94e+30     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -9.98e-08    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.39e+30     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1968         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 1997         |\n",
      "|    total_timesteps      | 3932160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.076272e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.122        |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.000492     |\n",
      "|    loss                 | 2.86e+30     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -1.88e-07    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.23e+30     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1965          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 2250          |\n",
      "|    total_timesteps      | 4423680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1232806e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.122         |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.000492      |\n",
      "|    loss                 | 5.85e+30      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -1.89e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.42e+30      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best parameters from the Optuna study\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Determine network architecture based on the best net_arch choice\n",
    "if best_params[\"net_arch\"] == \"small\":\n",
    "    net_arch = [64, 64]\n",
    "elif best_params[\"net_arch\"] == \"medium\":\n",
    "    net_arch = [256, 256]\n",
    "else:\n",
    "    net_arch = [400, 300]\n",
    "\n",
    "policy_kwargs = dict(net_arch=net_arch)\n",
    "\n",
    "# Instantiate PPO model with best parameters\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    clip_range=best_params[\"clip_range\"],\n",
    "    ent_coef=best_params[\"ent_coef\"],\n",
    "    vf_coef=best_params[\"vf_coef\"],\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "# Optionally, evaluate before training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    ppo_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False\n",
    ")\n",
    "evaluation_results['PPO_pre_training'] = {\n",
    "    'mean_reward': mean_reward,\n",
    "    'std_reward': std_reward\n",
    "}\n",
    "\n",
    "# Train the model using the tuned hyperparameters\n",
    "ppo_model.learn(total_timesteps=num_total_steps)\n",
    "\n",
    "# Evaluate after training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    ppo_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False\n",
    ")\n",
    "evaluation_results['PPO_post_training'] = {\n",
    "    'mean_reward': mean_reward,\n",
    "    'std_reward': std_reward\n",
    "}\n",
    "\n",
    "# Save the trained model\n",
    "ppo_model.save(\"ppo_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 17:53:54,890] A new study created in memory with name: no-name-578585ee-be5c-44c1-bfad-9706e83b012c\n",
      "/tmp/ipykernel_399/3498605266.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "/tmp/ipykernel_399/3498605266.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
      "/tmp/ipykernel_399/3498605266.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
      "/tmp/ipykernel_399/3498605266.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "[I 2025-03-27 17:53:57,243] Trial 0 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.0005611486243724771, 'gamma': 0.9466489255195275, 'ent_coef': 0.00010431195785404368, 'vf_coef': 0.8696092546430276, 'net_arch': 'small'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:53:59,579] Trial 1 finished with value: -1055.136589 and parameters: {'learning_rate': 1.761527576782793e-05, 'gamma': 0.961439415829525, 'ent_coef': 7.592984831474136e-06, 'vf_coef': 0.43859309414012804, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:54:01,913] Trial 2 finished with value: 11234.110917666667 and parameters: {'learning_rate': 1.992966832090082e-05, 'gamma': 0.9452658444252356, 'ent_coef': 0.0003280823862709786, 'vf_coef': 0.973182990319387, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:04,245] Trial 3 finished with value: -1018.563502 and parameters: {'learning_rate': 0.00030667803320479694, 'gamma': 0.9946191028472052, 'ent_coef': 3.974694227319247e-07, 'vf_coef': 0.7143909662889707, 'net_arch': 'large'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:06,584] Trial 4 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.0005826743210336183, 'gamma': 0.9659383805220103, 'ent_coef': 3.0791634360798274e-08, 'vf_coef': 0.6940109537294249, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:08,927] Trial 5 finished with value: -1024.3789333333332 and parameters: {'learning_rate': 0.00012833070980928585, 'gamma': 0.9948467494970316, 'ent_coef': 1.2329073861896357e-07, 'vf_coef': 0.7260057204525724, 'net_arch': 'medium'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:11,246] Trial 6 finished with value: -1026.956427 and parameters: {'learning_rate': 1.8246615060815577e-05, 'gamma': 0.9184977139341386, 'ent_coef': 7.767618230362513e-06, 'vf_coef': 0.7657773891639814, 'net_arch': 'medium'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:13,598] Trial 7 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.00015487178190200658, 'gamma': 0.982903735268294, 'ent_coef': 1.107257279671072e-08, 'vf_coef': 0.9832905407509434, 'net_arch': 'medium'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:15,924] Trial 8 finished with value: -1017.3209110000001 and parameters: {'learning_rate': 5.647710192792019e-05, 'gamma': 0.9566698384797475, 'ent_coef': 0.00013576735493551006, 'vf_coef': 0.28814327735233825, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:18,277] Trial 9 finished with value: -1025.377651 and parameters: {'learning_rate': 0.00031532465756177886, 'gamma': 0.9520573034453799, 'ent_coef': 0.0007006619955192509, 'vf_coef': 0.983512398569391, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:20,631] Trial 10 finished with value: -1039.3597416666667 and parameters: {'learning_rate': 4.3518151676486847e-05, 'gamma': 0.9245186903878375, 'ent_coef': 0.009221877914190041, 'vf_coef': 0.11118055416914313, 'net_arch': 'large'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:22,987] Trial 11 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.0009632675071328941, 'gamma': 0.9346312468817506, 'ent_coef': 0.00013864090174263714, 'vf_coef': 0.8780275160544868, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:25,292] Trial 12 finished with value: 5105.378712333332 and parameters: {'learning_rate': 1.0632098243502855e-05, 'gamma': 0.9042338557043318, 'ent_coef': 0.001239295533822288, 'vf_coef': 0.5371475110790174, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:27,577] Trial 13 finished with value: -3161.3648553333333 and parameters: {'learning_rate': 1.0393157047387112e-05, 'gamma': 0.9095445183653564, 'ent_coef': 0.0086993859693037, 'vf_coef': 0.52346004379007, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:29,862] Trial 14 finished with value: -1051.9521883333334 and parameters: {'learning_rate': 3.032996032107014e-05, 'gamma': 0.9025482031378501, 'ent_coef': 0.0009686809390609129, 'vf_coef': 0.37498029440266134, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:32,157] Trial 15 finished with value: -1088.9016353333334 and parameters: {'learning_rate': 1.0215579039437764e-05, 'gamma': 0.9388434975671636, 'ent_coef': 0.0008912212155603019, 'vf_coef': 0.5965684473338329, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:34,462] Trial 16 finished with value: -1036.4383726666667 and parameters: {'learning_rate': 2.2517278158567026e-05, 'gamma': 0.9741233847233979, 'ent_coef': 1.9350878361806245e-05, 'vf_coef': 0.5338112353365388, 'net_arch': 'large'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:36,766] Trial 17 finished with value: 5404.182701 and parameters: {'learning_rate': 7.505668339770199e-05, 'gamma': 0.9294263092001437, 'ent_coef': 0.002029915662884004, 'vf_coef': 0.22519766893341864, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:39,085] Trial 18 finished with value: -1032.8472016666667 and parameters: {'learning_rate': 7.157621484385822e-05, 'gamma': 0.9333852521427124, 'ent_coef': 2.9924015593101378e-05, 'vf_coef': 0.10704302787090039, 'net_arch': 'small'}. Best is trial 2 with value: 11234.110917666667.\n",
      "[I 2025-03-27 17:54:41,407] Trial 19 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 7.774371998860964e-05, 'gamma': 0.9233016770538768, 'ent_coef': 0.0032403960680700873, 'vf_coef': 0.24836011414329234, 'net_arch': 'large'}. Best is trial 2 with value: 11234.110917666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best A2C trial: {'learning_rate': 1.992966832090082e-05, 'gamma': 0.9452658444252356, 'ent_coef': 0.0003280823862709786, 'vf_coef': 0.973182990319387, 'net_arch': 'small'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def optimize_a2c(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 1e-8, 1e-2)\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0.1, 1.0)\n",
    "    \n",
    "    net_arch_choice = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"])\n",
    "    if net_arch_choice == \"small\":\n",
    "        net_arch = [64, 64]\n",
    "    elif net_arch_choice == \"medium\":\n",
    "        net_arch = [256, 256]\n",
    "    else:\n",
    "        net_arch = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=net_arch)\n",
    "    \n",
    "    model = A2C(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        policy_kwargs=policy_kwargs\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=40000)\n",
    "    \n",
    "    mean_reward, _ = evaluate_policy(model, Monitor(testing_env_fns[0]()), n_eval_episodes=3, deterministic=False)\n",
    "    return mean_reward\n",
    "\n",
    "study_a2c = optuna.create_study(direction=\"maximize\")\n",
    "study_a2c.optimize(optimize_a2c, n_trials=20)\n",
    "print(\"Best A2C trial:\", study_a2c.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17250    |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.2e+12  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 9.75e+25 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17166    |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 4.9e+13  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.64e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17141    |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 2.17e+13 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.64e+28 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17138     |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 480000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 1.99e-05  |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -1.22e+14 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.36e+29  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17111     |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 600000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 1.99e-05  |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -5.59e+13 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.13e+30  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17091    |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 2.12e+14 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 1.48e+30 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17061     |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 840000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 1.99e-05  |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -5.13e+12 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 4.11e+29  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 17039     |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 960000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 1.99e-05  |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.37e+14 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.08e+30  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17006    |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 3.74e+13 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 8.19e+28 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16983    |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 5.68e+13 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 1.55e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16965    |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 77       |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.56e+14 |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 2.93e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16950    |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.88e+14 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 1.36e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16937    |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 1560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 5.19e+13 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 9.48e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16931    |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.28e+13 |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 4.03e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16925    |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 106      |\n",
      "|    total_timesteps    | 1800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 2.78e+14 |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 1.48e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16922    |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 113      |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 1.66e+14 |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 2.56e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16925    |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 2040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 6.18e+14 |\n",
      "|    std                | 0.985    |\n",
      "|    value_loss         | 6.07e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16932    |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 2160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1.91e+14 |\n",
      "|    std                | 0.984    |\n",
      "|    value_loss         | 3.11e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16937    |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 2280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 5.84e+14 |\n",
      "|    std                | 0.983    |\n",
      "|    value_loss         | 5.53e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16939    |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 141      |\n",
      "|    total_timesteps    | 2400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.66e+14 |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 1.33e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16950    |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 2520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 1.3e+14  |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 1.2e+30  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 16961     |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 2640000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 1.99e-05  |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -4.44e+12 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 2.17e+29  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16973    |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 2760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 1.04e+14 |\n",
      "|    std                | 0.978    |\n",
      "|    value_loss         | 1.34e+30 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16979    |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 2880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 6.98e+13 |\n",
      "|    std                | 0.977    |\n",
      "|    value_loss         | 3.53e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16987    |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 176      |\n",
      "|    total_timesteps    | 3000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 4.5e+11  |\n",
      "|    std                | 0.977    |\n",
      "|    value_loss         | 1.78e+25 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 16994    |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 183      |\n",
      "|    total_timesteps    | 3120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 5.09e+13 |\n",
      "|    std                | 0.977    |\n",
      "|    value_loss         | 1.62e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17003    |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 190      |\n",
      "|    total_timesteps    | 3240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 4.28e+12 |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 6.58e+26 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17010    |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 197      |\n",
      "|    total_timesteps    | 3360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 5.15e+12 |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 3.52e+27 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17018    |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 204      |\n",
      "|    total_timesteps    | 3480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 2.2e+12  |\n",
      "|    std                | 0.975    |\n",
      "|    value_loss         | 2.73e+26 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17026    |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 211      |\n",
      "|    total_timesteps    | 3600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 3.67e+11 |\n",
      "|    std                | 0.975    |\n",
      "|    value_loss         | 1.28e+25 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17035    |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 218      |\n",
      "|    total_timesteps    | 3720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 3.08e+13 |\n",
      "|    std                | 0.975    |\n",
      "|    value_loss         | 2.59e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17045    |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 225      |\n",
      "|    total_timesteps    | 3840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 2.24e+12 |\n",
      "|    std                | 0.974    |\n",
      "|    value_loss         | 7.34e+26 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17055    |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 232      |\n",
      "|    total_timesteps    | 3960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 3.99e+12 |\n",
      "|    std                | 0.974    |\n",
      "|    value_loss         | 5.31e+27 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17061    |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 239      |\n",
      "|    total_timesteps    | 4080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 3.18e+13 |\n",
      "|    std                | 0.974    |\n",
      "|    value_loss         | 1.17e+29 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 17065    |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 246      |\n",
      "|    total_timesteps    | 4200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 1.99e-05 |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 1.57e+11 |\n",
      "|    std                | 0.973    |\n",
      "|    value_loss         | 8.16e+23 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best parameters from the A2C Optuna study (assumed to be stored in study_a2c)\n",
    "best_params_a2c = study_a2c.best_trial.params\n",
    "\n",
    "# Determine network architecture based on the best net_arch choice\n",
    "if best_params_a2c[\"net_arch\"] == \"small\":\n",
    "    net_arch = [64, 64]\n",
    "elif best_params_a2c[\"net_arch\"] == \"medium\":\n",
    "    net_arch = [256, 256]\n",
    "else:\n",
    "    net_arch = [400, 300]\n",
    "\n",
    "policy_kwargs = dict(net_arch=net_arch)\n",
    "\n",
    "# Instantiate A2C model with best parameters\n",
    "a2c_model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=best_params_a2c[\"learning_rate\"],\n",
    "    gamma=best_params_a2c[\"gamma\"],\n",
    "    ent_coef=best_params_a2c[\"ent_coef\"],\n",
    "    vf_coef=best_params_a2c[\"vf_coef\"],\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Evaluate before training\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['A2C_pre_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "\n",
    "# Train the A2C model\n",
    "a2c_model.learn(total_timesteps=num_total_steps)\n",
    "\n",
    "# Evaluate after training\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['A2C_post_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "\n",
    "# Save the trained model\n",
    "a2c_model.save(\"a2c_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 17:58:50,412] A new study created in memory with name: no-name-ac1ecb9e-1d84-4a28-8113-a41713b1ecbb\n",
      "/tmp/ipykernel_399/3177982610.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_399/3177982610.py:17: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  tau = trial.suggest_uniform(\"tau\", 0.001, 0.02)\n",
      "/tmp/ipykernel_399/3177982610.py:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
      "[I 2025-03-27 17:58:53,504] Trial 0 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 5.664914102625951e-05, 'tau': 0.00229544956332761, 'gamma': 0.983226108713692, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:58:56,545] Trial 1 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 1.7322932512924224e-05, 'tau': 0.012303650928764339, 'gamma': 0.9753459509867434, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:03,052] Trial 2 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 0.006337199107528451, 'tau': 0.007401260593336118, 'gamma': 0.9934248415487626, 'batch_size': 64, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:09,541] Trial 3 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 0.0001260026092911416, 'tau': 0.019376139653971763, 'gamma': 0.9947537593655574, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:12,544] Trial 4 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.002173247914546497, 'tau': 0.012163615804313532, 'gamma': 0.9744778940356187, 'batch_size': 64, 'net_arch': 'small'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:15,501] Trial 5 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 6.715666119699164e-05, 'tau': 0.009153214324036037, 'gamma': 0.9174434870499737, 'batch_size': 128, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:21,956] Trial 6 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 0.0003110524193283396, 'tau': 0.016912655066113728, 'gamma': 0.9376276811860824, 'batch_size': 128, 'net_arch': 'small'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:28,523] Trial 7 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 0.0027436616104339804, 'tau': 0.011543123549360972, 'gamma': 0.9492550445708846, 'batch_size': 128, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:31,580] Trial 8 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.00013427416667663808, 'tau': 0.0182443114200554, 'gamma': 0.9020617045171859, 'batch_size': 128, 'net_arch': 'small'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:34,560] Trial 9 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.0011026211294181, 'tau': 0.003864625252152944, 'gamma': 0.9969302072392251, 'batch_size': 64, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:41,066] Trial 10 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 1.2055033650547586e-05, 'tau': 0.0010673545096458925, 'gamma': 0.9705997222288283, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:47,617] Trial 11 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 1.7078186709076403e-05, 'tau': 0.014503551394513575, 'gamma': 0.9729539974331052, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:54,070] Trial 12 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 3.370405600072134e-05, 'tau': 0.006057935051229763, 'gamma': 0.9595776993941904, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 17:59:57,107] Trial 13 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 3.6299654885981536e-05, 'tau': 0.014429086830279714, 'gamma': 0.9831801997554656, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 18:00:03,662] Trial 14 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 1.05796690715692e-05, 'tau': 0.001593910462124655, 'gamma': 0.9556176635851924, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 18:00:06,715] Trial 15 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 4.602909770419924e-05, 'tau': 0.009188978017915772, 'gamma': 0.9401023590157426, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 18:00:13,239] Trial 16 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 0.00038122175792666425, 'tau': 0.005773315979219007, 'gamma': 0.9823759304152467, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 18:00:19,700] Trial 17 finished with value: -1043833.6292539999 and parameters: {'learning_rate': 0.00010170756254406989, 'tau': 0.014013893763818192, 'gamma': 0.9626284095163463, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 18:00:22,668] Trial 18 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 2.2059319352943514e-05, 'tau': 0.003544585622547101, 'gamma': 0.9845560648084588, 'batch_size': 256, 'net_arch': 'medium'}. Best is trial 0 with value: -1011.6246146666666.\n",
      "[I 2025-03-27 18:00:25,597] Trial 19 finished with value: -1011.6246146666666 and parameters: {'learning_rate': 0.0003514654102430986, 'tau': 0.01013182509989726, 'gamma': 0.9436967004333193, 'batch_size': 256, 'net_arch': 'large'}. Best is trial 0 with value: -1011.6246146666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value:  -1011.6246146666666\n",
      "Params: \n",
      "  learning_rate: 5.664914102625951e-05\n",
      "  tau: 0.00229544956332761\n",
      "  gamma: 0.983226108713692\n",
      "  batch_size: 256\n",
      "  net_arch: medium\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "n_actions = vec_env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(\n",
    "    mean=np.zeros(n_actions), \n",
    "    sigma=0.2 * np.ones(n_actions)\n",
    ")\n",
    "\n",
    "def optimize_ddpg(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    tau = trial.suggest_uniform(\"tau\", 0.001, 0.02)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.90, 0.999)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    # Choose network architecture preset\n",
    "    net_arch_choice = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"large\"])\n",
    "    if net_arch_choice == \"small\":\n",
    "        net_arch = [64, 64]\n",
    "    elif net_arch_choice == \"medium\":\n",
    "        net_arch = [256, 256]\n",
    "    else:\n",
    "        net_arch = [400, 300]\n",
    "        \n",
    "    policy_kwargs = dict(net_arch=net_arch)\n",
    "    \n",
    "    # Create DDPG model with sampled hyperparameters.\n",
    "    model = DDPG(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        action_noise=action_noise\n",
    "    )\n",
    "    \n",
    "    # Use fewer timesteps for speed; adjust as needed.\n",
    "    model.learn(total_timesteps=40000)\n",
    "    \n",
    "    # Evaluate the policy on a Monitor-wrapped environment.\n",
    "    mean_reward, _ = evaluate_policy(model, Monitor(testing_env_fns[0]()), n_eval_episodes=3, deterministic=True)\n",
    "    return mean_reward\n",
    "\n",
    "study_ddpg = optuna.create_study(direction=\"maximize\")\n",
    "study_ddpg.optimize(optimize_ddpg, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study_ddpg.best_trial\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best parameters from the study\n",
    "best_trial = study_ddpg.best_trial\n",
    "\n",
    "learning_rate = best_trial.params[\"learning_rate\"]\n",
    "tau = best_trial.params[\"tau\"]\n",
    "gamma = best_trial.params[\"gamma\"]\n",
    "batch_size = best_trial.params[\"batch_size\"]\n",
    "net_arch_choice = best_trial.params[\"net_arch\"]\n",
    "\n",
    "if net_arch_choice == \"small\":\n",
    "    net_arch = [64, 64]\n",
    "elif net_arch_choice == \"medium\":\n",
    "    net_arch = [256, 256]\n",
    "else:\n",
    "    net_arch = [400, 300]\n",
    "\n",
    "policy_kwargs = dict(net_arch=net_arch)\n",
    "\n",
    "# Initialize action noise\n",
    "n_actions = vec_env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(\n",
    "    mean=np.zeros(n_actions), \n",
    "    sigma=0.2 * np.ones(n_actions)  # Adjust sigma to tune exploration\n",
    ")\n",
    "\n",
    "# Instantiate DDPG model with the best hyperparameters\n",
    "ddpg_model = DDPG(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=learning_rate,\n",
    "    tau=tau,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise\n",
    ")\n",
    "# Train third agent with DDPG\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(ddpg_model, Monitor(testing_env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['DDPG_pre_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "ddpg_model.learn(total_timesteps=num_total_steps)\n",
    "mean_reward, std_reward = evaluate_policy(ddpg_model, Monitor(env_fns[0]()), n_eval_episodes=5, deterministic=False)\n",
    "evaluation_results['DDPG_post_training'] = {'mean_reward': mean_reward, 'std_reward': std_reward}\n",
    "ddpg_model.save(\"ddpg_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PPO_pre_training': {'mean_reward': np.float64(-1038.241156), 'std_reward': np.float64(23.69553647218389)}, 'PPO_post_training': {'mean_reward': np.float64(-2851.6011142), 'std_reward': np.float64(3021.08023416297)}, 'A2C_pre_training': {'mean_reward': np.float64(1544.5351720000003), 'std_reward': np.float64(6680.084945885893)}, 'A2C_post_training': {'mean_reward': np.float64(103089534233.76855), 'std_reward': np.float64(129455765727.34544)}, 'DDPG_pre_training': {'mean_reward': np.float64(12520398.652432), 'std_reward': np.float64(15335534.293922234)}, 'DDPG_post_training': {'mean_reward': np.float64(173748697.0455372), 'std_reward': np.float64(31615.285385608673)}}\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, algorithm='rule', model=None, horizon=48, soc_resolution=20):\n",
    "        \"\"\"\n",
    "        env: an instance of SolarBatteryEnv.\n",
    "        algorithm: choose between 'rule', 'rl', 'dt', or 'sdp'.\n",
    "        model: For RL algorithm, a trained model with a predict method (e.g., from stable_baselines3).\n",
    "        horizon: Time horizon for SDP optimization (default: 48 steps).\n",
    "        soc_resolution: Resolution of state-of-charge discretization (default: 20 levels).\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.algorithm = algorithm.lower()\n",
    "        self.model = model\n",
    "        self.horizon = horizon\n",
    "        self.soc_resolution = soc_resolution\n",
    "        self.value_function = None\n",
    "        self.policy = None\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        if self.algorithm == 'rule':\n",
    "            return self.rule_based_action(obs)\n",
    "        elif self.algorithm == 'rl':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"RL algorithm selected but no model provided.\")\n",
    "            obs_batch = obs[None, ...] if isinstance(obs, np.ndarray) else obs\n",
    "            action, _ = self.model.predict(obs_batch, deterministic=True)\n",
    "            return action[0] if isinstance(action, np.ndarray) and action.ndim > 1 else action\n",
    "        elif self.algorithm == 'dt':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"Decision Transformer selected but no model provided.\")\n",
    "            device = next(self.model.parameters()).device\n",
    "            state = torch.tensor(obs, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
    "            rtg = torch.tensor([[0.0]], dtype=torch.float32, device=device).reshape(1, 1, 1)\n",
    "            timestep = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "            actions = torch.zeros((1, 1, self.model.act_dim), dtype=torch.float32, device=device)\n",
    "            _, _, act_preds = self.model(state, rtg, timestep, actions)\n",
    "            action = act_preds[0, 0].detach().cpu().numpy().tolist()\n",
    "            return action\n",
    "        elif self.algorithm == 'sdp':\n",
    "            if self.value_function is None or self.policy is None:\n",
    "                self.value_function, self.policy = self._sdp_optimization()\n",
    "            return self.sdp_action(obs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Algorithm '{self.algorithm}' is not supported.\")\n",
    "\n",
    "    def rule_based_action(self, obs):\n",
    "        battery_level = obs[-2]\n",
    "        capacity = self.env.battery_capacity\n",
    "        if battery_level < capacity * 0.2:\n",
    "            return [1.0]\n",
    "        elif battery_level > capacity * 0.8:\n",
    "            return [-1.0]\n",
    "        else:\n",
    "            return [0.0]\n",
    "\n",
    "    def sdp_action(self, obs):\n",
    "        \"\"\"\n",
    "        Computes the optimal action using the precomputed SDP policy.\n",
    "        Observation must contain the current battery level.\n",
    "        \"\"\"\n",
    "        battery_level = obs[-2]\n",
    "        soc_states = np.linspace(0, self.env.battery_capacity, self.soc_resolution)\n",
    "        closest_state_idx = np.argmin(np.abs(soc_states - battery_level))\n",
    "        return [self.policy[0, closest_state_idx]]\n",
    "\n",
    "    def _sdp_optimization(self):\n",
    "        \"\"\"\n",
    "        Computes the value function and policy using SDP optimization.\n",
    "        \"\"\"\n",
    "        soc_states = np.linspace(0, self.env.battery_capacity, self.soc_resolution)\n",
    "        value_function = np.zeros((self.horizon, len(soc_states)))\n",
    "        policy = np.zeros((self.horizon, len(soc_states)))\n",
    "\n",
    "        for t in reversed(range(self.horizon)):\n",
    "            for i, soc in enumerate(soc_states):\n",
    "                costs = []\n",
    "                actions = []\n",
    "                for action in np.linspace(-self.env.max_battery_flow, self.env.max_battery_flow, 20):\n",
    "                    battery_flow_energy = action * self.env.step_duration\n",
    "                    if action < 0 and abs(battery_flow_energy) > soc:\n",
    "                        continue\n",
    "                    if action > 0 and soc + battery_flow_energy > self.env.battery_capacity:\n",
    "                        continue\n",
    "\n",
    "                    soc_next = soc + battery_flow_energy\n",
    "                    soc_next = np.clip(soc_next, 0, self.env.battery_capacity)\n",
    "                    next_value = 0 if t == self.horizon - 1 else np.interp(soc_next, soc_states, value_function[t + 1])\n",
    "\n",
    "                    avg_soc = (soc - 0.5 * (-battery_flow_energy)) / self.env.battery_capacity * 100\n",
    "                    degradation_cost = self.env._calculate_battery_degradation(battery_flow_energy, avg_soc) * self.env.battery_life_cost\n",
    "                    grid_energy = battery_flow_energy + self.env.df['HouseLoad'][t] - self.env.df['SolarGen'][t]\n",
    "                    energy_price = self.env.df['ImportEnergyPrice'][t] if battery_flow_energy >= 0 else self.env.df['ExportEnergyPrice'][t]\n",
    "                    grid_reward, _ = self.env._calculate_grid_reward(grid_energy, energy_price)\n",
    "                    total_cost = -grid_reward + degradation_cost + next_value\n",
    "\n",
    "                    costs.append(total_cost)\n",
    "                    actions.append(action)\n",
    "\n",
    "                if costs:\n",
    "                    best_action_idx = np.argmin(costs)\n",
    "                    value_function[t, i] = costs[best_action_idx]\n",
    "                    policy[t, i] = actions[best_action_idx]\n",
    "\n",
    "        return value_function, policy\n",
    "\n",
    "    def run_episode(self, render=False):\n",
    "        obs, _ = self.env.reset()\n",
    "        logs = []\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            action = self.choose_action(obs)\n",
    "            next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            logs.append({\n",
    "                'observation': obs.tolist() if isinstance(obs, np.ndarray) else obs,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'info': info\n",
    "            })\n",
    "            obs = next_obs\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "        return pl.DataFrame(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, algorithm='rule', model=None):\n",
    "        \"\"\"\n",
    "        env: an instance of SolarBatteryEnv.\n",
    "        algorithm: choose between 'rule' for a rule‑based agent or 'rl' for reinforcement learning.\n",
    "        model: For RL algorithm, a trained model with a predict method (e.g., from stable_baselines3).\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.algorithm = algorithm.lower()\n",
    "        self.model = model\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        if self.algorithm == 'rule':\n",
    "            return self.rule_based_action(obs)\n",
    "        elif self.algorithm == 'rl':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"RL algorithm selected but no model provided.\")\n",
    "            # Ensure observation has the right shape for the model (e.g., batch dimension)\n",
    "            obs_batch = obs[None, ...] if isinstance(obs, np.ndarray) else obs\n",
    "            action, _ = self.model.predict(obs_batch, deterministic=True)\n",
    "            # Remove batch dimension if applicable.\n",
    "            return action[0] if isinstance(action, np.ndarray) and action.ndim > 1 else action\n",
    "        elif self.algorithm == 'dt':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"Decision Transformer selected but no model provided.\")\n",
    "            # Prepare inputs for the Decision Transformer.\n",
    "            # Assumes the current observation corresponds to a single timestep.\n",
    "            device = next(self.model.parameters()).device\n",
    "            state = torch.tensor(obs, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
    "            # Dummy return-to-go (rtg), here as 0; this should be set appropriately.\n",
    "            rtg = torch.tensor([[0.0]], dtype=torch.float32, device=device).reshape(1, 1, 1)\n",
    "            # Dummy timestep (set to 0 for the first step, update as needed).\n",
    "            timestep = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "            # Dummy previous action: zeros with the correct dimension. It is assumed that your model has an attribute act_dim.\n",
    "            actions = torch.zeros((1, 1, self.model.act_dim), dtype=torch.float32, device=device)\n",
    "            # Forward pass through the Decision Transformer.\n",
    "            _, _, act_preds = self.model(state, rtg, timestep, actions)\n",
    "            # Extract action from the predicted actions.\n",
    "            action = act_preds[0, 0].detach().cpu().numpy().tolist()\n",
    "            return action\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Algorithm '{self.algorithm}' is not supported.\")\n",
    "\n",
    "    def rule_based_action(self, obs):\n",
    "        \"\"\"\n",
    "        Simple heuristic based on the battery level. It assumes that the observation's\n",
    "        second-to-last element corresponds to the battery level.\n",
    "        \"\"\"\n",
    "        battery_level = obs[-2]\n",
    "        capacity = self.env.battery_capacity\n",
    "        # Rule: If battery level is below 20% of capacity, charge;\n",
    "        #       if above 80%, discharge; otherwise, hold.\n",
    "        if battery_level < capacity * 0.2:\n",
    "            return [1.0]  # Full charge action.\n",
    "        elif battery_level > capacity * 0.8:\n",
    "            return [-1.0]  # Full discharge action.\n",
    "        else:\n",
    "            return [0.0]  # Hold / no operation.\n",
    "\n",
    "    def run_episode(self, render=False):\n",
    "        \"\"\"\n",
    "        Runs one episode on the environment.\n",
    "        render: if True, call env.render() after every step.\n",
    "        Returns a Polars DataFrame with the observations, actions, and rewards.\n",
    "        \"\"\"\n",
    "        obs, _ = self.env.reset()\n",
    "        logs = []\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            action = self.choose_action(obs)\n",
    "            next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            logs.append({\n",
    "                'observation': obs.tolist() if isinstance(obs, np.ndarray) else obs,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'info': info\n",
    "            })\n",
    "            obs = next_obs\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "        return pl.DataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = testing_env_fns[0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ddpg model and create an agent\n",
    "#ddpg_model = DDPG.load(\"ddpg_agent\")\n",
    "agent = Agent(env, algorithm='rl', model=ppo_model)\n",
    "\n",
    "# run an episode with the agent\n",
    "episode_logs = agent.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Timestamp', 'SolarGen', 'HouseLoad', 'FutureSolar', 'FutureLoad', 'ImportEnergyPrice', 'ExportEnergyPrice', 'BatteryLevel', 'BatteryDegCost']\n",
      "shape: (9,)\n",
      "Series: '' [f64]\n",
      "[\n",
      "\t1.3150e15\n",
      "\t0.0\n",
      "\t1.101\n",
      "\t0.006\n",
      "\t3.626\n",
      "\t0.1\n",
      "\t0.08\n",
      "\t13.5\n",
      "\t-19743.75\n",
      "]\n",
      "0.005879707634449005\n",
      "{'reward_info': {'battery_charge': 0.0, 'battery_discharge': 0, 'demand': 1.101, 'supply': 0.0, 'grid_energy': 1.101, 'energy_price': 0.1, 'grid_violation_penalty': 0, 'grid_reward': -0.1101, 'battery_deg_penalty': -1.2904411554336548, 'battery_life_cost': 15300, 'final_reward': 19743.6399}}\n"
     ]
    }
   ],
   "source": [
    "print(env.get_observation_header())\n",
    "step = len(episode_logs) - 2\n",
    "print(episode_logs[step][\"observation\"][0])\n",
    "print(episode_logs[step][\"action\"][0])\n",
    "print(episode_logs[step][\"info\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
